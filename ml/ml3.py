# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.2
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3

# %% [markdown]
# # 100 Sports Image Classification

# %% [markdown]
# ## Google Drive mount for Google Colab
# %%
# from google.colab import drive
# drive.mount('/content/drive')

# %% [markdown]
# ## for Google Colab
# %%
# # !pwd
# # !pip install tensorflow
# # !nvidia-smi

# %% [markdown]
# ## ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# %%
# import kagglehub
# path = kagglehub.dataset_download("gpiosenka/sports-classification")
# print("Path to dataset files:", path)

# %% [markdown]
# ## Config
# %%
EPOCHS = 100
IMG_SIZE = 224
BATCH_SIZE = 32
LEARNING_RATE = 0.001

# %% [markdown]
# ## Import
# %%
import datetime
import json
import os
import pandas as pd

# %% [markdown]
# ## ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º

# %%
# data_path = '/content/drive/MyDrive/python/_test_cnn/archive/' # for google colab
# data_path = '/content/drive/MyDrive/python/_test_cnn/archive_mini/' # for google colab
data_path = './kagglehub_cache/datasets/gpiosenka/sports-classification/versions/9/'
# data_path = "./test_data/"
csv_path = data_path + "sports.csv"
df = pd.read_csv(csv_path)
df.head()

# %% [markdown]
# ### ã‚«ãƒ†ã‚´ãƒªåˆ—ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ãƒã‚§ãƒƒã‚¯
# %%
categorical_columns = ["data set"]
for col in categorical_columns:
    if col in df.columns:
        unique_values = df[col].value_counts()
        print(f"{col} åˆ—ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ ({len(unique_values)} å€‹):")
        for value, count in unique_values.items():
            print(f"  {value}: {count} ä»¶")

# %% [markdown]
# ## ãƒ‡ãƒ¼ã‚¿ã®åˆ†é›¢
# %%
df_train = df[df["data set"] == "train"]
df_test = df[df["data set"] == "test"]
df_valid = df[df["data set"] == "valid"]

# %% [markdown]
# ## CNN
# %%
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# %% [markdown]
# ### ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
# %%
train_datagen = ImageDataGenerator(
    rescale=1.0 / 255,
    rotation_range=15,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2],
    validation_split=0.2,  # 20%ã‚’validationã«åˆ†å‰²
)

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼ˆ80%ï¼‰
train_set = train_datagen.flow_from_directory(
    data_path + "train",
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    subset="training",  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ã‚µãƒ–ã‚»ãƒƒãƒˆ
)

# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆ20%ï¼‰- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãªã—
valid_datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.2)
valid_set = valid_datagen.flow_from_directory(
    data_path + "train",  # åŒã˜trainãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰åˆ†å‰²
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    subset="validation",  # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚µãƒ–ã‚»ãƒƒãƒˆ
    shuffle=False,
)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå…ƒã®testãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ç”¨ï¼‰
test_datagen = ImageDataGenerator(rescale=1.0 / 255)
test_set = test_datagen.flow_from_directory(
    data_path + "test",
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False,
)

# %% [markdown]
# ### ã‚¯ãƒ©ã‚¹æ•°ã®æ¤œå‡º
# %%
OUTPUT_LAYER_SIZE = len(train_set.class_indices)
print(f"æ¤œå‡ºã•ã‚ŒãŸã‚¯ãƒ©ã‚¹æ•°: {OUTPUT_LAYER_SIZE}")

# %% [markdown]
# ### ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ç¢ºèª
# %%
print("\n===== ã‚¯ãƒ©ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèª =====")
print("train:", train_set.class_indices)
print("valid:", valid_set.class_indices)
print("test :", test_set.class_indices)

print("\n===== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºç¢ºèª =====")
print(f"train samples: {train_set.samples} (trainãƒ•ã‚©ãƒ«ãƒ€ã®80%)")
print(f"valid samples: {valid_set.samples} (trainãƒ•ã‚©ãƒ«ãƒ€ã®20%)")
print(f"test samples: {test_set.samples} (å…ƒã®testãƒ•ã‚©ãƒ«ãƒ€)")
print(f"batch size: {BATCH_SIZE}")
print(f"valid batches per epoch: {len(valid_set)}")
print(f"train batches per epoch: {len(train_set)}")

# validationã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã®ç¢ºèª
if valid_set.samples >= 200:
    print(f"\nâœ… Validationãƒ‡ãƒ¼ã‚¿ãŒå……åˆ†ã‚ã‚Šã¾ã™ï¼ˆ{valid_set.samples}æšï¼‰")
    print("   - å®‰å®šã—ãŸè©•ä¾¡ãŒæœŸå¾…ã§ãã¾ã™")
elif valid_set.samples >= 100:
    print(f"\nğŸŸ¡ Validationãƒ‡ãƒ¼ã‚¿ã¯æœ€ä½é™ã‚ã‚Šã¾ã™ï¼ˆ{valid_set.samples}æšï¼‰")
    print("   - è©•ä¾¡ã¯ã‚ã‚‹ç¨‹åº¦å®‰å®šã—ã¾ã™")
else:
    print(
        f"\nâš ï¸  WARNING: Validationãƒ‡ãƒ¼ã‚¿ãŒã¾ã å°‘ãªã„ã§ã™ï¼ˆ{valid_set.samples}æšï¼‰"
    )
    print("   - ã‚ˆã‚Šå¤šãã®trainãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“")

# %% [markdown]
# ### CNNã®æ§‹ç¯‰
# %%
cnn = tf.keras.models.Sequential(
    [
        # input layer
        tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),
        # Block 1
        tf.keras.layers.Conv2D(32, 3, activation="relu", padding="same"),
        tf.keras.layers.Conv2D(32, 3, activation="relu", padding="same"),
        tf.keras.layers.MaxPooling2D(2),
        tf.keras.layers.BatchNormalization(),

        # Block 2
        tf.keras.layers.Conv2D(64, 3, activation="relu", padding="same"),
        tf.keras.layers.Conv2D(64, 3, activation="relu", padding="same"),
        tf.keras.layers.MaxPooling2D(2),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.25),

        # Block 3
        tf.keras.layers.Conv2D(128, 3, activation="relu", padding="same"),
        tf.keras.layers.Conv2D(128, 3, activation="relu", padding="same"),
        tf.keras.layers.MaxPooling2D(2),
        tf.keras.layers.BatchNormalization(),

        # Block 4
        tf.keras.layers.Conv2D(256, 3, activation="relu", padding="same"),
        tf.keras.layers.Conv2D(256, 3, activation="relu", padding="same"),
        tf.keras.layers.MaxPooling2D(2),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.25),

        # Dense layers
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(1024, activation="relu"),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),

        # Output layer
        tf.keras.layers.Dense(units=OUTPUT_LAYER_SIZE, activation="softmax"),
    ]
)

# %% [markdown]
# ### ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
# %%
cnn.compile(
    optimizer=tf.keras.optimizers.Adam(
        learning_rate=LEARNING_RATE
    ),
    loss="categorical_crossentropy",
    metrics=[
        tf.keras.metrics.CategoricalAccuracy(name="accuracy"),
        tf.keras.metrics.TopKCategoricalAccuracy(k=3, name="top3_acc"),
        tf.keras.metrics.TopKCategoricalAccuracy(k=5, name="top5_acc"),
    ],
)

# %% [markdown]
# ### Early Stopping
# %%
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss", patience=15, restore_best_weights=True, verbose=1
)


# %% [markdown]
# ### å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
# %%
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss", factor=0.5, patience=7, min_lr=1e-7, verbose=1
)

# %% [markdown]
# ### ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®æ¦‚è¦
# %%
print("\n===== ãƒ¢ãƒ‡ãƒ«æ§‹é€  =====")
cnn.summary()

print("\n===== å­¦ç¿’è¨­å®š =====")
print(f"å­¦ç¿’ç‡: {LEARNING_RATE}")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}")
print(f"ç”»åƒã‚µã‚¤ã‚º: {IMG_SIZE}x{IMG_SIZE}")
print(f"æœ€å¤§ã‚¨ãƒãƒƒã‚¯: {EPOCHS}")

# %% [markdown]
# ### å­¦ç¿’ã®å®Ÿè¡Œ
# %%
history = cnn.fit(
    x=train_set,
    validation_data=valid_set,
    epochs=EPOCHS,
    callbacks=[early_stopping, reduce_lr],
    verbose=1,
)

# %% [markdown]
# ### è©•ä¾¡ã¨ä¿å­˜ï¼ˆï¼‹åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã¨æ··åŒè¡Œåˆ—ï¼‰
# %%
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# %%
eval_results = cnn.evaluate(valid_set)
print("\n===== è©•ä¾¡çµæœ (evaluate) =====")
metrics_report = dict(zip(cnn.metrics_names, eval_results))
for name, value in metrics_report.items():
    print(f"{name}: {value:.4f}")

print("\n===== æœ€çµ‚ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆhistoryãƒ™ãƒ¼ã‚¹ï¼‰=====")
for metric in ["val_accuracy", "val_top3_acc", "val_top5_acc"]:
    val_list = history.history.get(metric)
    if val_list:
        print(f"{metric} : {val_list[-1]:.4f}")
    else:
        print(f"{metric} : æŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# æ­£è§£ãƒ©ãƒ™ãƒ«ã¨äºˆæ¸¬ãƒ©ãƒ™ãƒ«
y_true = valid_set.classes
y_pred_prob = cnn.predict(valid_set, verbose=0)
y_pred = np.argmax(y_pred_prob, axis=1)

# åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
print("\n===== Classification Report =====")
print(classification_report(y_true, y_pred, target_names=list(valid_set.class_indices.keys())))

# æ··åŒè¡Œåˆ—
print("\n===== Confusion Matrix =====")
cm = confusion_matrix(y_true, y_pred)
print(cm)

# å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=False, cmap="Blues", fmt="d")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()
plt.show()

# %% [markdown]
# ### ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
# %%
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
save_dir = os.path.join("./ml/model", f"run_{timestamp}")
os.makedirs(save_dir, exist_ok=True)

with open(os.path.join(save_dir, "class_indices.json"), "w") as f:
    json.dump(train_set.class_indices, f, indent=2, ensure_ascii=False)

cnn.save(os.path.join(save_dir, "model.keras"))
cnn.save(os.path.join(save_dir, "model.h5"))

config = {
    "epochs": EPOCHS,
    "img_size": IMG_SIZE,
    "batch_size": BATCH_SIZE,
    "output_layer_size": OUTPUT_LAYER_SIZE,
    "save_format": ["keras", "h5"],
    "metrics": metrics_report,
}
with open(os.path.join(save_dir, "config.json"), "w") as f:
    json.dump(config, f, indent=2, ensure_ascii=False)

print("\nãƒ¢ãƒ‡ãƒ«ãƒ»è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚ä¿å­˜å…ˆ:", save_dir)