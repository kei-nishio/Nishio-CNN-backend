{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38cc390",
   "metadata": {},
   "source": [
    "# 100 Sports Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788725b",
   "metadata": {},
   "source": [
    "## データのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a39e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"gpiosenka/sports-classification\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d0268",
   "metadata": {},
   "source": [
    "## 設定値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = (\n",
    "    10  # エポック数（初期テスト：10~30、軽量モデル：30~50、本格学習：50~100）\n",
    ")\n",
    "IMG_SIZE = 128  # 画像のサイズ（224は転移学習向けの標準。ConvNet自作では128程度がバランス良）\n",
    "BATCH_SIZE = 64  # バッチサイズ（32はメモリ節約・訓練遅め、64は訓練高速化・学習安定性あり）\n",
    "OUTPUT_LAYER_SIZE = 10  # 出力層のサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39f444",
   "metadata": {},
   "source": [
    "## データの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dce2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class id</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "      <th>data set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/001.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/002.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/003.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/004.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/005.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class id                 filepaths      labels data set\n",
       "0         0  train/air hockey/001.jpg  air hockey    train\n",
       "1         0  train/air hockey/002.jpg  air hockey    train\n",
       "2         0  train/air hockey/003.jpg  air hockey    train\n",
       "3         0  train/air hockey/004.jpg  air hockey    train\n",
       "4         0  train/air hockey/005.jpg  air hockey    train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data_path = './kagglehub_cache/datasets/gpiosenka/sports-classification/versions/9/'\n",
    "data_path = \"./test_data/\"\n",
    "csv_path = data_path + \"sports.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0af79",
   "metadata": {},
   "source": [
    "### カテゴリ列のユニーク値チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01942c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set 列のユニーク値 (3 個):\n",
      "  train: 13493 件\n",
      "  test: 500 件\n",
      "  valid: 500 件\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = [\"data set\"]\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        unique_values = df[col].value_counts()\n",
    "        print(f\"{col} 列のユニーク値 ({len(unique_values)} 個):\")\n",
    "        for value, count in unique_values.items():\n",
    "            print(f\"  {value}: {count} 件\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee87d3",
   "metadata": {},
   "source": [
    "## データの分離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894512f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class id</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "      <th>data set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/001.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/002.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/003.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/004.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>train/air hockey/005.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class id                 filepaths      labels data set\n",
       "0         0  train/air hockey/001.jpg  air hockey    train\n",
       "1         0  train/air hockey/002.jpg  air hockey    train\n",
       "2         0  train/air hockey/003.jpg  air hockey    train\n",
       "3         0  train/air hockey/004.jpg  air hockey    train\n",
       "4         0  train/air hockey/005.jpg  air hockey    train"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = df[df[\"data set\"] == \"train\"]\n",
    "df_test = df[df[\"data set\"] == \"test\"]\n",
    "df_valid = df[df[\"data set\"] == \"valid\"]\n",
    "\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98665c19",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e19bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7985932",
   "metadata": {},
   "source": [
    "### データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad5d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1289 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True\n",
    ")\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    data_path + \"train\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True\n",
    ")\n",
    "valid_set = valid_datagen.flow_from_directory(\n",
    "    data_path + \"valid\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True\n",
    ")\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    data_path + \"test\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359837e",
   "metadata": {},
   "source": [
    "### CNNの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        input_shape=[IMG_SIZE, IMG_SIZE, 3],\n",
    "    )\n",
    ")\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "cnn.add(tf.keras.layers.Dropout(0.3))\n",
    "cnn.add(tf.keras.layers.Dense(units=OUTPUT_LAYER_SIZE, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e46c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=\"top3_acc\"),\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top5_acc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fc3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569ms/step - accuracy: 0.0867 - loss: 2.9443 - top3_acc: 0.2989 - top5_acc: 0.4997"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 610ms/step - accuracy: 0.1040 - loss: 2.6012 - top3_acc: 0.3142 - top5_acc: 0.5252 - val_accuracy: 0.1000 - val_loss: 2.2567 - val_top3_acc: 0.3000 - val_top5_acc: 0.5600\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 552ms/step - accuracy: 0.1389 - loss: 2.2046 - top3_acc: 0.3949 - top5_acc: 0.6144 - val_accuracy: 0.2600 - val_loss: 2.0923 - val_top3_acc: 0.4400 - val_top5_acc: 0.6000\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 553ms/step - accuracy: 0.2816 - loss: 1.9736 - top3_acc: 0.5919 - top5_acc: 0.7665 - val_accuracy: 0.3600 - val_loss: 1.7225 - val_top3_acc: 0.7400 - val_top5_acc: 0.8800\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 555ms/step - accuracy: 0.4081 - loss: 1.7343 - top3_acc: 0.7269 - top5_acc: 0.8619 - val_accuracy: 0.5600 - val_loss: 1.3866 - val_top3_acc: 0.8000 - val_top5_acc: 0.8800\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 558ms/step - accuracy: 0.4973 - loss: 1.4811 - top3_acc: 0.7843 - top5_acc: 0.9015 - val_accuracy: 0.6200 - val_loss: 1.1854 - val_top3_acc: 0.9200 - val_top5_acc: 0.9800\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 557ms/step - accuracy: 0.5749 - loss: 1.3181 - top3_acc: 0.8285 - top5_acc: 0.9302 - val_accuracy: 0.6800 - val_loss: 1.0621 - val_top3_acc: 0.8600 - val_top5_acc: 0.9800\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 538ms/step - accuracy: 0.5795 - loss: 1.2651 - top3_acc: 0.8534 - top5_acc: 0.9395 - val_accuracy: 0.6600 - val_loss: 1.0096 - val_top3_acc: 0.9000 - val_top5_acc: 0.9400\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 547ms/step - accuracy: 0.6276 - loss: 1.1344 - top3_acc: 0.8681 - top5_acc: 0.9496 - val_accuracy: 0.7000 - val_loss: 0.9711 - val_top3_acc: 0.9200 - val_top5_acc: 0.9800\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 561ms/step - accuracy: 0.6656 - loss: 1.0339 - top3_acc: 0.8937 - top5_acc: 0.9620 - val_accuracy: 0.6600 - val_loss: 0.8479 - val_top3_acc: 0.8800 - val_top5_acc: 0.9800\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 533ms/step - accuracy: 0.6734 - loss: 0.9572 - top3_acc: 0.9077 - top5_acc: 0.9659 - val_accuracy: 0.8200 - val_loss: 0.7621 - val_top3_acc: 0.9200 - val_top5_acc: 0.9600\n"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(x=train_set, validation_data=valid_set, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbebe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.7400 - loss: 0.7875 - top3_acc: 0.9200 - top5_acc: 0.9400\n",
      "loss: 0.7875\n",
      "compile_metrics: 0.7400\n"
     ]
    }
   ],
   "source": [
    "results = cnn.evaluate(valid_set)\n",
    "for name, value in zip(cnn.metrics_names, results):\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b137c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "クラスインデックス（class_indices）を JSON 出力しました。\n",
      "モデルを保存しました: model_10epochs.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "class_indices = train_set.class_indices\n",
    "model_path = \"./ml\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(model_path, \"class_indices.json\"), \"w\") as f:\n",
    "    json.dump(class_indices, f, indent=2, ensure_ascii=False)\n",
    "print(\"クラスインデックス（class_indices）を JSON 出力しました。\")\n",
    "\n",
    "model_filename = f\"model_{EPOCHS}epochs.h5\"\n",
    "cnn.save(os.path.join(model_path, model_filename))\n",
    "print(f\"モデルを保存しました: {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
